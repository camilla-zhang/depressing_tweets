{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Trtk6lvxl0s"
      },
      "source": [
        "# Import Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9oMTAHOzriz",
        "outputId": "d2f0a382-236e-4c31-909e-eb3eef959800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.8/dist-packages (3.10.0)\n",
            "Collecting tweepy\n",
            "  Downloading tweepy-4.12.1-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (1.24.3)\n",
            "Installing collected packages: requests, tweepy\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "Successfully installed requests-2.28.2 tweepy-4.12.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.5/104.5 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop_words\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=03df6edf368d2dac7dac41b377f85a4f8e6e1631580920469afb6a0362e6f864\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/03/0d/3bd31c983789aeb0b4d5e2ca48590288d9db1586cf5f225062\n",
            "Successfully built stop_words\n",
            "Installing collected packages: stop_words\n",
            "Successfully installed stop_words-2018.7.23\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vaderSentiment) (2.28.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install tweepy --upgrade\n",
        "!pip install contractions\n",
        "!pip install nltk\n",
        "!pip install stop_words\n",
        "!pip install vaderSentiment\n",
        "\n",
        "import tweepy\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/My Drive/Colab Notebooks')\n",
        "import config\n",
        "\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from stop_words import get_stop_words\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import gensim.downloader as api\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "!pip install tensorflow-text\n",
        "!pip install tensorflow\n",
        "!pip install keras-preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras import optimizers\n",
        "import keras_tuner\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.metrics import BinaryAccuracy, Precision, Recall\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWaHRMSJJOT-",
        "outputId": "ca411d3c-9ce0-4a50-8eb8-55e30fd7aaa8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.2.1-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.6/169.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (7.9.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.28.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.30.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (23.1.21)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.1.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (4.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (15.0.6.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.51.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.38.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.2.2)\n",
            "Installing collected packages: kt-legacy, jedi, keras-tuner\n",
            "Successfully installed jedi-0.18.2 keras-tuner-1.2.1 kt-legacy-1.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.51.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (4.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (2.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (23.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (23.1.21)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (15.0.6.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.30.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.28.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from keras-preprocessing) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras-preprocessing) (1.21.6)\n",
            "Installing collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mute warnings\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)"
      ],
      "metadata": {
        "id": "0UfHWumsnM8c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import twitter data\n",
        "df3 = pd.read_csv('/content/gdrive/MyDrive/depressed_tweets.csv')\n",
        "non_depressed = pd.read_csv('/content/gdrive/MyDrive/non_depressed.csv')\n",
        "rdf2 = pd.read_csv('/content/gdrive/MyDrive/random_tweets.csv')"
      ],
      "metadata": {
        "id": "HQ0c8aqEnOLj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append datasets\n",
        "rdf2 = rdf2[0:len(df3)-len(non_depressed)] # remove some tweets to balance data\n",
        "non_depressed_tweets = rdf2[['tweet', 'depressed']].append(non_depressed[['tweet', 'depressed']])\n",
        "tweets_df = df3[['tweet', 'depressed']].append(non_depressed_tweets)"
      ],
      "metadata": {
        "id": "BIrUsw2FWDTB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show balanced data\n",
        "tweets_df.depressed.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quQCFNdZuIqt",
        "outputId": "76e71e2b-c7ab-48a1-949b-f8de29ea104a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1467\n",
              "0    1467\n",
              "Name: depressed, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "The data preprocessing includes removing punctuation, lemmatizing tweets, and removing stopwords. Among the stopwords are the keywords used to identify tweets that indicate depression. "
      ],
      "metadata": {
        "id": "Y0sdL7V6UQV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "tweets_df[\"tweet\"] = tweets_df['tweet'].str.replace('[^\\w\\s]|[0-9]','')\n",
        "tweets_df.head()\n",
        "\n",
        "# Tokenization #\n",
        "tweets_df['tweet'] = tweets_df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
        "\n",
        "# Stem words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_tweets = []\n",
        "for tweet in tweets_df['tweet']:\n",
        "  lemmatized_tweet = []\n",
        "  for word in tweet:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)\n",
        "    lemmatized_tweet.append(lemmatized_word)\n",
        "  lemmatized_tweets.append(lemmatized_tweet)\n",
        "tweets_df['tweet'] = lemmatized_tweets\n",
        "\n",
        "## Stopwords ##\n",
        "\n",
        "# Combine get_stopwords with NLTK stop words\n",
        "stop_words = ['feel', \"like\", 'depressed', 'depression', 'anxiety', 'antidepressant', 'antidepressants', 'feeling', 'felt'] #We used these words to search for tweets, thus we end up removing tweets\n",
        "stop_words2 = list(get_stop_words('en'))\n",
        "stop_words.extend(stop_words2)\n",
        "nltk_stop_words = list(stopwords.words('english'))\n",
        "stop_words.extend(nltk_stop_words)\n",
        "\n",
        "# Remove stop words\n",
        "sentences_list = []\n",
        "for word_list in tweets_df['tweet']:\n",
        "  filtered_sentence = []\n",
        "  for word in word_list:\n",
        "    if word not in stop_words:\n",
        "      filtered_sentence.append(word)\n",
        "  sentences_list.append(filtered_sentence)\n",
        "tweets_df['tweet'] = sentences_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIntmj91dhkz",
        "outputId": "c1101af8-b9a3-4513-83d7-1c9501bbb30a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-401eb70c9192>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  tweets_df[\"tweet\"] = tweets_df['tweet'].str.replace('[^\\w\\s]|[0-9]','')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tweets back into text \n",
        "detokenized_tweets = []\n",
        "for tweet in tweets_df.tweet:\n",
        "  detokenized_tweets.append(TreebankWordDetokenizer().detokenize(tweet))\n",
        "tweets_df['text'] = detokenized_tweets"
      ],
      "metadata": {
        "id": "eKuEor1RJq_O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to list\n",
        "x = tweets_df['text'].to_list()\n",
        "y = tweets_df['depressed'].to_list()"
      ],
      "metadata": {
        "id": "87f-ut4vo1FG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment"
      ],
      "metadata": {
        "id": "LsDUpyCstVuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append negative, neutral, positive, and composite sentiment scores to twitter data\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer() # Instantiate sentiment\n",
        "tweets_df['sentiment_score'] = tweets_df['text'].apply(sentiment_analyzer.polarity_scores)\n",
        "tweets_df = pd.concat([tweets_df.drop(['sentiment_score'], axis=1), tweets_df['sentiment_score'].apply(pd.Series)], axis=1)\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "okiOJAFXvvZ8",
        "outputId": "19db0e36-b815-44ea-e02f-cd756dfd5aa8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               tweet  depressed  \\\n",
              "0  [working, home, today, making, corporate, smal...          1   \n",
              "1                       [need, one, night, go, skin]          1   \n",
              "2            [think, even, excited, antman, anymore]          1   \n",
              "3  [fucking, extrovert, get, horribly, regular, i...          1   \n",
              "4  [severely, think, thing, help, current, state,...          1   \n",
              "\n",
              "                                                text    neg    neu    pos  \\\n",
              "0  working home today making corporate small task...  0.000  0.851  0.149   \n",
              "1                             need one night go skin  0.000  1.000  0.000   \n",
              "2                  think even excited antman anymore  0.000  0.625  0.375   \n",
              "3  fucking extrovert get horribly regular interac...  0.508  0.492  0.000   \n",
              "4  severely think thing help current state mall p...  0.219  0.584  0.197   \n",
              "\n",
              "   compound  \n",
              "0    0.2732  \n",
              "1    0.0000  \n",
              "2    0.3400  \n",
              "3   -0.6326  \n",
              "4   -0.0772  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbe2562d-940c-43b9-a99a-39be3507d9f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>depressed</th>\n",
              "      <th>text</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[working, home, today, making, corporate, smal...</td>\n",
              "      <td>1</td>\n",
              "      <td>working home today making corporate small task...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.851</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.2732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[need, one, night, go, skin]</td>\n",
              "      <td>1</td>\n",
              "      <td>need one night go skin</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[think, even, excited, antman, anymore]</td>\n",
              "      <td>1</td>\n",
              "      <td>think even excited antman anymore</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.3400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[fucking, extrovert, get, horribly, regular, i...</td>\n",
              "      <td>1</td>\n",
              "      <td>fucking extrovert get horribly regular interac...</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.6326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[severely, think, thing, help, current, state,...</td>\n",
              "      <td>1</td>\n",
              "      <td>severely think thing help current state mall p...</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.584</td>\n",
              "      <td>0.197</td>\n",
              "      <td>-0.0772</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbe2562d-940c-43b9-a99a-39be3507d9f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbe2562d-940c-43b9-a99a-39be3507d9f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbe2562d-940c-43b9-a99a-39be3507d9f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the average sentiment scores by tweet classification\n",
        "tweets_df.groupby(['depressed']).mean('compound')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "RHISFdNsuzUH",
        "outputId": "34f895f3-dd68-4fbd-d177-5bd5b8d9e8fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                neg       neu       pos  compound\n",
              "depressed                                        \n",
              "0          0.123261  0.693561  0.182497  0.076347\n",
              "1          0.189804  0.636767  0.170024 -0.019408"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12de6d30-eace-46c2-8840-4117120b7287\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>depressed</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.123261</td>\n",
              "      <td>0.693561</td>\n",
              "      <td>0.182497</td>\n",
              "      <td>0.076347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.189804</td>\n",
              "      <td>0.636767</td>\n",
              "      <td>0.170024</td>\n",
              "      <td>-0.019408</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12de6d30-eace-46c2-8840-4117120b7287')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12de6d30-eace-46c2-8840-4117120b7287 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12de6d30-eace-46c2-8840-4117120b7287');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding"
      ],
      "metadata": {
        "id": "0qoToKwB55ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Vectorizer (Bag of Words)\n",
        "\n",
        "Takes into account the word frequencies in each tweet"
      ],
      "metadata": {
        "id": "-MIfTkEkTHOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate and fit/transform on count vectorizer\n",
        "x_bow = CountVectorizer().fit_transform(x)\n",
        "\n",
        "# Test and train split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_bow, y, stratify = y, random_state=11)"
      ],
      "metadata": {
        "id": "bOSkmhl1kag9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF\n",
        "\n",
        "Considers the important of a word in a tweet in relation to the total number of times the word appears in\n"
      ],
      "metadata": {
        "id": "TqqM2HL9sq4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_tfidf = TfidfVectorizer().fit_transform(x)\n",
        "\n",
        "# Test and train split\n",
        "tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test = train_test_split(x_tfidf, y, stratify = y, random_state=11)"
      ],
      "metadata": {
        "id": "pFcGGcK5skdg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GloVe\n",
        "\n",
        "Uses a matrix factorization technique to account for the co-occurance between words at a global level\n"
      ],
      "metadata": {
        "id": "6aKLI__CTJin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word vectorizer from twitter\n",
        "wv = api.load('glove-twitter-50')"
      ],
      "metadata": {
        "id": "32iHJFYo_FYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0626122c-94c4-4b57-b93c-b38e51a376b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert text into vectorized form\n",
        "def glove_vectorizer(text):\n",
        "  vector_size = wv.vector_size\n",
        "  wv_res = np.zeros(vector_size)\n",
        "  count = 1\n",
        "  for word in text:\n",
        "    if word in wv:\n",
        "      count +=1\n",
        "      wv_res += wv[word]\n",
        "  wv_res = wv_res/count\n",
        "  return wv_res \n",
        "    \n",
        "# Apply function to text\n",
        "tweets_df['glove'] = tweets_df['text'].apply(glove_vectorizer)\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "kDKK1YTweDMH",
        "outputId": "3dfea413-9871-4f30-bacb-2057bc3559be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               tweet  depressed  \\\n",
              "0  [working, home, today, making, corporate, smal...          1   \n",
              "1                       [need, one, night, go, skin]          1   \n",
              "2            [think, even, excited, antman, anymore]          1   \n",
              "3  [fucking, extrovert, get, horribly, regular, i...          1   \n",
              "4  [severely, think, thing, help, current, state,...          1   \n",
              "\n",
              "                                                text    neg    neu    pos  \\\n",
              "0  working home today making corporate small task...  0.000  0.851  0.149   \n",
              "1                             need one night go skin  0.000  1.000  0.000   \n",
              "2                  think even excited antman anymore  0.000  0.625  0.375   \n",
              "3  fucking extrovert get horribly regular interac...  0.508  0.492  0.000   \n",
              "4  severely think thing help current state mall p...  0.219  0.584  0.197   \n",
              "\n",
              "   compound                                              glove  \n",
              "0    0.2732  [0.23994614269870979, 0.08996781630393787, 0.1...  \n",
              "1    0.0000  [0.36984789018568237, -0.17828817096979996, 0....  \n",
              "2    0.3400  [0.27604273209969205, -0.05447451062500477, 0....  \n",
              "3   -0.6326  [0.260807118652498, 0.12523734985905535, 0.148...  \n",
              "4   -0.0772  [0.32182383696463973, 0.0006920882924036547, 0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12f276f2-89ef-4b8f-9126-c42d60a9c8c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>depressed</th>\n",
              "      <th>text</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "      <th>glove</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[working, home, today, making, corporate, smal...</td>\n",
              "      <td>1</td>\n",
              "      <td>working home today making corporate small task...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.851</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>[0.23994614269870979, 0.08996781630393787, 0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[need, one, night, go, skin]</td>\n",
              "      <td>1</td>\n",
              "      <td>need one night go skin</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[0.36984789018568237, -0.17828817096979996, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[think, even, excited, antman, anymore]</td>\n",
              "      <td>1</td>\n",
              "      <td>think even excited antman anymore</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.3400</td>\n",
              "      <td>[0.27604273209969205, -0.05447451062500477, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[fucking, extrovert, get, horribly, regular, i...</td>\n",
              "      <td>1</td>\n",
              "      <td>fucking extrovert get horribly regular interac...</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.6326</td>\n",
              "      <td>[0.260807118652498, 0.12523734985905535, 0.148...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[severely, think, thing, help, current, state,...</td>\n",
              "      <td>1</td>\n",
              "      <td>severely think thing help current state mall p...</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.584</td>\n",
              "      <td>0.197</td>\n",
              "      <td>-0.0772</td>\n",
              "      <td>[0.32182383696463973, 0.0006920882924036547, 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12f276f2-89ef-4b8f-9126-c42d60a9c8c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12f276f2-89ef-4b8f-9126-c42d60a9c8c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12f276f2-89ef-4b8f-9126-c42d60a9c8c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test and train split\n",
        "x2 = tweets_df['glove'].to_list()\n",
        "y2 = tweets_df['depressed'].to_list()\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, stratify = y2, random_state=11)"
      ],
      "metadata": {
        "id": "7G3Bfk1oRj30"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "1rlGjJ3FDMRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Count Vectorizer"
      ],
      "metadata": {
        "id": "BJLzCtm7t_ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Grid Searching ##\n",
        "\n",
        "#Get CV Score for Logistic Regression\n",
        "log = LogisticRegression(max_iter = 1000)\n",
        "\n",
        "params = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'penalty': ['none', 'l1', 'l2']}\n",
        "grid = GridSearchCV(log, param_grid=params, cv=5)\n",
        "\n",
        "#use meta model methods to fit score and predict model:\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
        "print(\"Logistic Count Vectorizer best mean CV score: {:.3f}\".format(grid.best_score_))\n",
        "print(\"Logistic Count Vectorizer best parameters: {}\".format(grid.best_params_))\n",
        "print(\"Logistic Count Vectorizer test-set score: {:.3f}\".format(grid.score(x_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHlmRYPhcPXk",
        "outputId": "09ad6561-2672-42a6-9f6f-03029884e6d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Count Vectorizer best mean CV score: 0.743\n",
            "Logistic Count Vectorizer best parameters: {'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Logistic Count Vectorizer test-set score: 0.749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "20 fits failed out of a total of 75.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.66090909 0.69454545        nan 0.71090909 0.72454545        nan\n",
            "        nan 0.73181818        nan 0.73272727 0.74227273 0.74272727\n",
            " 0.74227273 0.74136364 0.74045455]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Score for Logistic Regression\n",
        "log = LogisticRegression(penalty= 'l2', solver= 'saga')\n",
        "log.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "a4TKOp0SE7vT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full report\n",
        "log_predicted = log.predict(x_test)\n",
        "print(\"Prediction report for Logistic Bag of Words Model ~\")\n",
        "print(classification_report(y_test, log_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RqEFTAHXXLZ",
        "outputId": "a6e32881-7daa-4e9e-c778-e865a1c2b6c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.73      0.74       367\n",
            "           1       0.74      0.75      0.74       367\n",
            "\n",
            "    accuracy                           0.74       734\n",
            "   macro avg       0.74      0.74      0.74       734\n",
            "weighted avg       0.74      0.74      0.74       734\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "z-y-topCuNDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search\n",
        "\n",
        "log = LogisticRegression(max_iter = 1000)\n",
        "params = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'penalty': ['none', 'l1', 'l2']}\n",
        "grid = GridSearchCV(log, param_grid=params, cv=5)\n",
        "\n",
        "# Use meta model methods to fit score and predict model:\n",
        "grid.fit(tfidf_x_train, tfidf_y_train)\n",
        "\n",
        "# Extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
        "print(\"Logistic TF IDF best mean CV score: {:.3f}\".format(grid.best_score_))\n",
        "print(\"Logistic TF IDF best parameters: {}\".format(grid.best_params_))\n",
        "print(\"Logistic TF IDF test-set score: {:.3f}\".format(grid.score(tfidf_x_test, tfidf_y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah5sM6JHisAb",
        "outputId": "e2df0965-4e6f-48da-e6f1-330d185f6d56"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic TF IDF best mean CV score: 0.760\n",
            "Logistic TF IDF best parameters: {'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "Logistic TF IDF test-set score: 0.770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "20 fits failed out of a total of 75.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.68863636 0.68863636        nan 0.69136364 0.695             nan\n",
            "        nan 0.72              nan 0.72       0.75954545 0.75954545\n",
            " 0.75909091 0.75954545 0.75954545]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log = LogisticRegression(penalty= 'l2', solver= 'newton-cg')\n",
        "log.fit(tfidf_x_train, tfidf_y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww0cCL3GuDuC",
        "outputId": "5a736050-fbd9-4e59-e850-3b54b486606f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic TF IDF Vectorizer Prediction Score ->  75.89158345221112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full report\n",
        "log2_predicted = log.predict(tfidf_x_test)\n",
        "print(\"Prediction report for Logistic TF IDF Model ~\")\n",
        "print(classification_report(tfidf_y_test, log2_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHFKDwZEXkq0",
        "outputId": "83c0dab7-e451-49c2-8ea2-a1f6765b0e35"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.81      0.78       367\n",
            "           1       0.80      0.72      0.76       367\n",
            "\n",
            "    accuracy                           0.77       734\n",
            "   macro avg       0.77      0.77      0.77       734\n",
            "weighted avg       0.77      0.77      0.77       734\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GloVe"
      ],
      "metadata": {
        "id": "_9GZpdWAuAdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search \n",
        "log = LogisticRegression(max_iter = 1000)\n",
        "params = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'penalty': ['none', 'l1', 'l2']}\n",
        "grid = GridSearchCV(log, param_grid=params, cv=5)\n",
        "\n",
        "# Fit score and predict model:\n",
        "grid.fit(x_train2, y_train2)\n",
        "\n",
        "# Extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
        "print(\"Logistic GloVe best mean CV score: {:.3f}\".format(grid.best_score_))\n",
        "print(\"Logistic GloVe best parameters: {}\".format(grid.best_params_))\n",
        "print(\"Logistic GloVe test set score: {:.3f}\".format(grid.score(x_test2, y_test2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g5tENvdjG9o",
        "outputId": "691de12f-14a6-49e6-94b5-a576ba6c99dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "20 fits failed out of a total of 75.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.58363636 0.585             nan 0.58636364 0.58636364        nan\n",
            "        nan 0.57818182        nan 0.575      0.58045455 0.58\n",
            " 0.57909091 0.58045455 0.58      ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic GloVe best mean CV score: 0.586\n",
            "Logistic GloVe best parameters: {'penalty': 'none', 'solver': 'sag'}\n",
            "Logistic GloVe test set score: 0.557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log = LogisticRegression(penalty= 'l2', solver= 'lbfgs')\n",
        "log.fit(x_train2, y_train2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MePksOGqCHWa",
        "outputId": "bdaf3dbe-a690-4a32-aab1-5538dd4cd717"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Glove Prediction Score ->  52.22381635581061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full report\n",
        "log3_predicted = log.predict(x_test2)\n",
        "print(\"Prediction report for Logistic GloVe Model ~\")\n",
        "print(classification_report(y_test2, log3_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CYvWkjVX1W-",
        "outputId": "5070f457-fbf0-404e-f927-2ebb1efeee6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.60      0.57       367\n",
            "           1       0.55      0.50      0.52       367\n",
            "\n",
            "    accuracy                           0.55       734\n",
            "   macro avg       0.55      0.55      0.55       734\n",
            "weighted avg       0.55      0.55      0.55       734\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine"
      ],
      "metadata": {
        "id": "zPFy2TDDKQGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Count Vectorizer (bag of words)"
      ],
      "metadata": {
        "id": "AIsHte64Kq-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Searching"
      ],
      "metadata": {
        "id": "REM2u5hPgyJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM GridSearch \n",
        "SVM = svm.SVC()\n",
        "params = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [100, 10, 1.0, 0.1, 0.001], 'gamma': [1,0.1,0.01,0.001]}\n",
        "grid = GridSearchCV(SVM, param_grid=params, cv=5)\n",
        "\n",
        "# Fit score and predict model:\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "print(\"SVM Count Vect best mean CV score: {:.3f}\".format(grid.best_score_))\n",
        "print(\"SVM Count Vect best parameters: {}\".format(grid.best_params_))\n",
        "print(\"SVM Count Vect test set score: {:.3f}\".format(grid.score(x_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY89dpd1dwPA",
        "outputId": "66ec4cda-f2c9-4693-a508-39c591fbe174"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Count Vect best mean CV score: 0.746\n",
            "SVM Count Vect best parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "SVM Count Vect test set score: 0.741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the training dataset and predict on text\n",
        "SVM = svm.SVC(C = 10, gamma = 0.01, kernel='rbf')\n",
        "SVM.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTAPEHVeKPkE",
        "outputId": "cead47e2-e2ad-4336-8a61-1365712f9227"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Count Vectorizer Prediction Score ->  73.01136363636364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full report\n",
        "svm_predicted = SVM.predict(x_test)\n",
        "print(\"Classification report for SVM Count Vectorizer Model ~\")\n",
        "print(classification_report(y_test, svm_predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtNwnMawX_Rs",
        "outputId": "05fb00b0-73b0-48dc-fa22-f1cf6e9ac690"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.78      0.75       367\n",
            "           1       0.76      0.70      0.73       367\n",
            "\n",
            "    accuracy                           0.74       734\n",
            "   macro avg       0.74      0.74      0.74       734\n",
            "weighted avg       0.74      0.74      0.74       734\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "NhfNPFZxsJgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF IDF GridSearch\n",
        "SVM = svm.SVC()\n",
        "params = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [100, 10, 1.0, 0.1, 0.001], 'gamma': [1,0.1,0.01,0.001]}\n",
        "grid = GridSearchCV(SVM, param_grid=params, cv=5)\n",
        "\n",
        "# Fit and predict model:\n",
        "grid.fit(tfidf_x_train, tfidf_y_train)\n",
        "\n",
        "print(\"SVM TF IDF best mean CV score: {:.3f}\".format(grid.best_score_))\n",
        "print(\"SVM TF IDF best parameters: {}\".format(grid.best_params_))\n",
        "print(\"SVM TF IDF test-set score: {:.3f}\".format(grid.score(tfidf_x_test, tfidf_y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TNTPPFrsIyJ",
        "outputId": "4a8a4a8c-f833-4fa0-f8b1-f958aa2f5694"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM TF IDF best mean CV score: 0.760\n",
            "SVM TF IDF best parameters: {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
            "SVM TF IDF test-set score: 0.760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C = 10, gamma = 1, kernel='rbf')\n",
        "SVM.fit(tfidf_x_train,tfidf_y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lUBUNR3hOMc",
        "outputId": "3b38c1ee-f4f6-41c8-fb83-5a4ecebdf8c1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM TF-IDF Vectorizer Prediction Score ->  75.2112676056338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full report\n",
        "svm_predicted2 = SVM.predict(tfidf_x_test)\n",
        "print(\"Classification report for SVM TF-IDF Model ~\")\n",
        "print(classification_report(tfidf_y_test, svm_predicted2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcWicuKYYHh9",
        "outputId": "84bc8376-49be-4ec2-97f9-11238603b06e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77       367\n",
            "           1       0.78      0.73      0.75       367\n",
            "\n",
            "    accuracy                           0.76       734\n",
            "   macro avg       0.76      0.76      0.76       734\n",
            "weighted avg       0.76      0.76      0.76       734\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Glove"
      ],
      "metadata": {
        "id": "LmT9nYB7MGpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Glove SVM GridSearch\n",
        "SVM = svm.SVC()\n",
        "params = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [100, 10, 1.0, 0.1, 0.001], 'gamma': [1,0.1,0.01,0.001]}\n",
        "grid = GridSearchCV(SVM, param_grid=params, cv=5)\n",
        "grid.fit(x_train2, y_train2)\n",
        "\n",
        "# Extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
        "print(\"SVM best mean CV score: {:.3f}\".format(grid.best_score_))\n",
        "print(\"SVM best parameters: {}\".format(grid.best_params_))\n",
        "print(\"SVM test-set score: {:.3f}\".format(grid.score(x_test2, y_test2)))"
      ],
      "metadata": {
        "id": "2rSe2_-Aj4JP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6615e4fa-f3bb-4471-c0a1-3a29019431d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM best mean CV score: 0.623\n",
            "SVM best parameters: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
            "SVM test-set score: 0.604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "SVM.fit(x_train2, y_train2)\n",
        "# Predict and score on test\n",
        "svm_predictions2 = SVM.predict(x_test2)"
      ],
      "metadata": {
        "id": "OArR_WbTMGJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52550bf8-6ceb-4916-f3e8-d3b0b605bbb8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM GloVe Score ->  52.67727930535456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full report\n",
        "print(\"Classification report for SVM GloVe Model ~\")\n",
        "svm_predicted3 = SVM.predict(x_test2)\n",
        "print(classification_report(y_test2, svm_predicted3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iU6SNMgYaHz",
        "outputId": "dac29f2c-1990-4644-a9ba-d4b1d256c0c4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.61      0.58       367\n",
            "           1       0.56      0.50      0.53       367\n",
            "\n",
            "    accuracy                           0.55       734\n",
            "   macro avg       0.56      0.55      0.55       734\n",
            "weighted avg       0.56      0.55      0.55       734\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks"
      ],
      "metadata": {
        "id": "-a3wNQzeZNwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras Embedding"
      ],
      "metadata": {
        "id": "p4mn_M5gV8Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x3 = tweets_df['text']\n",
        "y3 = tweets_df['depressed']"
      ],
      "metadata": {
        "id": "05GUKH58bcmW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 100 # Defines the top n number of words in the dictionary\n",
        "max_length = 10 # Length of each sequence\n",
        " \n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(tweets_df['text'])\n",
        "sequences = tokenizer.texts_to_sequences(tweets_df['text']) # Process the tweets into sequences\n",
        "\n",
        "x3 = pad_sequences(sequences, maxlen=max_length) # Padding sequences converts characters to the same length\n",
        "\n",
        "#Test and train split\n",
        "x_train3, x_test3, y_train3, y_test3 = train_test_split(x3, tweets_df['depressed'], stratify = tweets_df['depressed'], random_state=11)"
      ],
      "metadata": {
        "id": "FWj3yC3ecLf6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 32\n",
        "\n",
        "# Build model\n",
        "model = Sequential() \n",
        "model.add(Embedding(input_dim = max_words, output_dim = output_dim, input_length=max_length)) # Dense classification layer\n",
        "model.add(Flatten()) # Reshape embedding output\n",
        "model.add(Dense(512, activation='relu')) #128 nodes <------- TUNE THIS\n",
        "model.add(Dense(1, activation='sigmoid')) # 2 classes\n",
        "\n",
        "# Fit and compile\n",
        "metrics = [\n",
        "    BinaryAccuracy(name='accuracy'),\n",
        "    Precision(name='precision'),\n",
        "    Recall(name='recall')]\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=metrics) \n",
        "\n",
        "results = model.fit(x_train3, \n",
        "                 y_train3, \n",
        "                 epochs=20, \n",
        "                 batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9JMXokxZOxs",
        "outputId": "0f133f0b-7499-4b57-87e7-7368078af0c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.6424 - accuracy: 0.6277 - precision: 0.6683 - recall: 0.5073\n",
            "Epoch 2/20\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 0.5784 - accuracy: 0.7036 - precision: 0.7129 - recall: 0.6818\n",
            "Epoch 3/20\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 0.5466 - accuracy: 0.7209 - precision: 0.7445 - recall: 0.6727\n",
            "Epoch 4/20\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 0.5275 - accuracy: 0.7368 - precision: 0.7639 - recall: 0.6855\n",
            "Epoch 5/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.5103 - accuracy: 0.7450 - precision: 0.7870 - recall: 0.6718\n",
            "Epoch 6/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4967 - accuracy: 0.7500 - precision: 0.7919 - recall: 0.6782\n",
            "Epoch 7/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7668 - precision: 0.8112 - recall: 0.6955\n",
            "Epoch 8/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4673 - accuracy: 0.7691 - precision: 0.8142 - recall: 0.6973\n",
            "Epoch 9/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4519 - accuracy: 0.7795 - precision: 0.8296 - recall: 0.7036\n",
            "Epoch 10/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.7868 - precision: 0.8448 - recall: 0.7027\n",
            "Epoch 11/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.7964 - precision: 0.8417 - recall: 0.7300\n",
            "Epoch 12/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4213 - accuracy: 0.7982 - precision: 0.8497 - recall: 0.7245\n",
            "Epoch 13/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4116 - accuracy: 0.7986 - precision: 0.8551 - recall: 0.7191\n",
            "Epoch 14/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8082 - precision: 0.8561 - recall: 0.7409\n",
            "Epoch 15/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.3922 - accuracy: 0.8132 - precision: 0.8692 - recall: 0.7373\n",
            "Epoch 16/20\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 0.3844 - accuracy: 0.8186 - precision: 0.8709 - recall: 0.7482\n",
            "Epoch 17/20\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 0.3801 - accuracy: 0.8205 - precision: 0.8730 - recall: 0.7500\n",
            "Epoch 18/20\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 0.3731 - accuracy: 0.8209 - precision: 0.8755 - recall: 0.7482\n",
            "Epoch 19/20\n",
            "69/69 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8314 - precision: 0.8785 - recall: 0.7691\n",
            "Epoch 20/20\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.8318 - precision: 0.8786 - recall: 0.7700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Net W/3 Hidden Layers + 1 Dropout"
      ],
      "metadata": {
        "id": "OFCNw-bqsXOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hp = keras_tuner.HyperParameters()\n",
        "metrics = [\n",
        "    BinaryAccuracy(name='accuracy'),\n",
        "    Precision(name='precision'),\n",
        "    Recall(name='recall')]"
      ],
      "metadata": {
        "id": "CxbzYbYOgDdM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x3 = tweets_df['text']\n",
        "y3 = tweets_df['depressed']\n",
        " \n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(x3)\n",
        "sequences = tokenizer.texts_to_sequences(x3) # Process the tweets into sequences\n",
        "\n",
        "x3 = pad_sequences(sequences, maxlen=max_length) # Padding sequences converts characters to the same length\n",
        "\n",
        "#Test and train split\n",
        "x_train3, x_test3, y_train3, y_test3 = train_test_split(x3, y3, stratify = y3, random_state=11)\n",
        "\n",
        "#Build model, compile, and fit\n",
        "output_dim = 32\n",
        "best_model = Sequential() \n",
        "best_model.add(Embedding(input_dim = max_words, output_dim = output_dim, input_length=max_length)) # Dense classification layer\n",
        "best_model.add(Flatten()) # Reshape embedding output\n",
        "best_model.add(Dense(units=hp.Choice('num_units', values=[16, 64, 32, 128, 256, 512, 1024, 2048], default=64), \n",
        "                activation='relu')) # tuning nodes\n",
        "best_model.add(Dense(units=hp.Choice('num_units', values=[16, 64, 32, 128, 256, 512, 1024, 2048], default=64), \n",
        "                activation='relu'))\n",
        "best_model.add(Dense(units=hp.Choice('num_units', values=[16, 64, 32, 128, 256, 512, 1024, 2048], default=64), \n",
        "                activation='relu'))       \n",
        "best_model.add(Dropout(0.2))\n",
        "best_model.add(Dense(1, activation='sigmoid')) # 2 classes\n",
        "\n",
        "best_model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=metrics) \n",
        "\n",
        "results = best_model.fit(x_train3, \n",
        "                 y_train3, \n",
        "                 epochs=20, \n",
        "                 batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGKB-mmRju-1",
        "outputId": "5957f80b-898e-4a76-e5e2-c45f0ec15f6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "69/69 [==============================] - 1s 2ms/step - loss: 0.6724 - accuracy: 0.5995 - precision: 0.6287 - recall: 0.4864\n",
            "Epoch 2/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.6895 - precision: 0.7134 - recall: 0.6336\n",
            "Epoch 3/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.5522 - accuracy: 0.7150 - precision: 0.7492 - recall: 0.6464\n",
            "Epoch 4/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.7468 - precision: 0.7843 - recall: 0.6809\n",
            "Epoch 5/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7573 - precision: 0.8004 - recall: 0.6855\n",
            "Epoch 6/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7686 - precision: 0.8160 - recall: 0.6936\n",
            "Epoch 7/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7773 - precision: 0.8245 - recall: 0.7045\n",
            "Epoch 8/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7905 - precision: 0.8410 - recall: 0.7164\n",
            "Epoch 9/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.7945 - precision: 0.8396 - recall: 0.7282\n",
            "Epoch 10/20\n",
            "69/69 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8014 - precision: 0.8592 - recall: 0.7209\n",
            "Epoch 11/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8077 - precision: 0.8582 - recall: 0.7373\n",
            "Epoch 12/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8123 - precision: 0.8620 - recall: 0.7436\n",
            "Epoch 13/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8223 - precision: 0.8736 - recall: 0.7536\n",
            "Epoch 14/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8227 - precision: 0.8777 - recall: 0.7500\n",
            "Epoch 15/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8409 - precision: 0.8898 - recall: 0.7782\n",
            "Epoch 16/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8318 - precision: 0.8810 - recall: 0.7673\n",
            "Epoch 17/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 0.8373 - precision: 0.8905 - recall: 0.7691\n",
            "Epoch 18/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8427 - precision: 0.8903 - recall: 0.7818\n",
            "Epoch 19/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8500 - precision: 0.8961 - recall: 0.7918\n",
            "Epoch 20/20\n",
            "69/69 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8364 - precision: 0.8830 - recall: 0.7755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BERT w/3 hidden layers, 1 dropout layer\n",
        "\n",
        "Treats the same words differently when in under different contexts"
      ],
      "metadata": {
        "id": "zMLJ4aKogJh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text\n",
        "bert_preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\" # URL to preprocess text\n",
        "encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\" #URL to encode\n",
        "\n",
        "bert_preprocess_model = hub.KerasLayer(bert_preprocess_url) # BERT preprocessor\n",
        "bert_encoder_model = hub.KerasLayer(encoder_url) # BERT encoder\n",
        "\n",
        "# Initialize input layer using BERT preprocessed text\n",
        "input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') # input layer using preprocessed text\n",
        "preprocessed_text = bert_preprocess_model(input)\n",
        "output = bert_encoder_model(preprocessed_text)\n",
        "# Initialize NN layers \n",
        "l = tf.keras.layers.Dense(units=hp.Choice('num_units', values=[16, 64, 32, 128, 256, 512, 1024, 2048], default=64), \n",
        "                activation='relu') # tuning nodes\n",
        "l = tf.keras.layers.Dense(units=hp.Choice('num_units', values=[16, 64, 32, 128, 256, 512, 1024, 2048], default=64), \n",
        "                activation='relu')\n",
        "l = tf.keras.layers.Dropout(0.2, name=\"dropout\")(output['pooled_output']) # drop nodes to prevent overfitting; input BERT layer\n",
        "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input], outputs = [l])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvyTDF_wOJL",
        "outputId": "a6aaaf2d-9a1a-4729-d351-93241fbd7fc0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       {'input_type_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     {'default': (None,   109482241   ['keras_layer[0][0]',            \n",
            "                                768),                             'keras_layer[0][1]',            \n",
            "                                 'sequence_output':               'keras_layer[0][2]']            \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768)}                                                       \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 769\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "x_train4, x_test4, y_train4, y_test4 = train_test_split(tweets_df['text'], tweets_df['depressed'], stratify = tweets_df['depressed'], random_state=11)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=metrics)\n",
        "\n",
        "results = model.fit(x_train4, \n",
        "                 y_train4, \n",
        "                 epochs=20, \n",
        "                 batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq4YbexhwT9E",
        "outputId": "42d094bc-d6e0-4ca8-9f33-2ee03cf1fa2d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "69/69 [==============================] - 1198s 17s/step - loss: 0.7083 - accuracy: 0.6848 - precision: 0.6950 - recall: 0.6586\n",
            "Epoch 2/20\n",
            "69/69 [==============================] - 1184s 17s/step - loss: 0.6871 - accuracy: 0.5650 - precision: 0.5613 - recall: 0.5955\n",
            "Epoch 3/20\n",
            "69/69 [==============================] - 1209s 18s/step - loss: 0.6666 - accuracy: 0.5905 - precision: 0.5840 - recall: 0.6291\n",
            "Epoch 4/20\n",
            "69/69 [==============================] - 1179s 17s/step - loss: 0.6411 - accuracy: 0.6300 - precision: 0.6295 - recall: 0.6318\n",
            "Epoch 5/20\n",
            "69/69 [==============================] - 1176s 17s/step - loss: 0.6450 - accuracy: 0.6259 - precision: 0.6185 - recall: 0.6573\n",
            "Epoch 6/20\n",
            "69/69 [==============================] - 1183s 17s/step - loss: 0.6351 - accuracy: 0.6414 - precision: 0.6328 - recall: 0.6736\n",
            "Epoch 7/20\n",
            "69/69 [==============================] - 1176s 17s/step - loss: 0.6296 - accuracy: 0.6468 - precision: 0.6438 - recall: 0.6573\n",
            "Epoch 8/20\n",
            "69/69 [==============================] - 1219s 18s/step - loss: 0.6193 - accuracy: 0.6564 - precision: 0.6458 - recall: 0.6927\n",
            "Epoch 9/20\n",
            "69/69 [==============================] - 1172s 17s/step - loss: 0.6203 - accuracy: 0.6486 - precision: 0.6433 - recall: 0.6673\n",
            "Epoch 10/20\n",
            "69/69 [==============================] - 1174s 17s/step - loss: 0.6057 - accuracy: 0.6695 - precision: 0.6587 - recall: 0.7036\n",
            "Epoch 11/20\n",
            "69/69 [==============================] - 1181s 17s/step - loss: 0.6099 - accuracy: 0.6768 - precision: 0.6675 - recall: 0.7045\n",
            "Epoch 12/20\n",
            "69/69 [==============================] - 1174s 17s/step - loss: 0.6115 - accuracy: 0.6586 - precision: 0.6535 - recall: 0.6755\n",
            "Epoch 13/20\n",
            "69/69 [==============================] - 1170s 17s/step - loss: 0.5989 - accuracy: 0.6782 - precision: 0.6759 - recall: 0.6845\n",
            "Epoch 14/20\n",
            "69/69 [==============================] - 1264s 18s/step - loss: 0.5991 - accuracy: 0.6755 - precision: 0.6708 - recall: 0.6891\n",
            "Epoch 15/20\n",
            "69/69 [==============================] - 1182s 17s/step - loss: 0.6079 - accuracy: 0.6664 - precision: 0.6548 - recall: 0.7036\n",
            "Epoch 16/20\n",
            "69/69 [==============================] - 1166s 17s/step - loss: 0.5894 - accuracy: 0.6886 - precision: 0.6812 - recall: 0.7091\n",
            "Epoch 17/20\n",
            "69/69 [==============================] - 1162s 17s/step - loss: 0.5899 - accuracy: 0.6845 - precision: 0.6747 - recall: 0.7127\n",
            "Epoch 18/20\n",
            "69/69 [==============================] - 1172s 17s/step - loss: 0.5904 - accuracy: 0.6809 - precision: 0.6730 - recall: 0.7036\n",
            "Epoch 19/20\n",
            "69/69 [==============================] - 1148s 17s/step - loss: 0.5928 - accuracy: 0.6809 - precision: 0.6755 - recall: 0.6964\n",
            "Epoch 20/20\n",
            "69/69 [==============================] - 1146s 17s/step - loss: 0.5855 - accuracy: 0.6873 - precision: 0.6817 - recall: 0.7027\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}